{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b868c6eb",
   "metadata": {},
   "source": [
    "## Download and check rdf2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edae41f5-8379-478a-80b4-8703c94916fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f4da3",
   "metadata": {},
   "source": [
    "## Download Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6a4166-a9ac-41ca-8c70-df8812e17253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fb15k-237_nt.zip\n"
     ]
    }
   ],
   "source": [
    "# download .nt dataset from my drive\n",
    "! wget -q -nc --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pBnn8bjI2VkVvBR33DnvpeyocfDhMCFA' -O fb15k-237_nt.zip\n",
    "\n",
    "! unzip -n fb15k-237_nt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9e650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def get_entities(graphs):\n",
    "    # get subjects and objects\n",
    "    entities = []\n",
    "    \n",
    "    for g in graphs:\n",
    "        entities = entities + list(g.subjects(unique=True)) + list(g.objects(unique=True))\n",
    "\n",
    "    # pythons stupid version of nub\n",
    "    entities = list(dict.fromkeys(entities))\n",
    "    return entities\n",
    "\n",
    "def get_all_corrupted_triples_fast(triple,entities,position = 'object'):\n",
    "    # not faster ...\n",
    "\n",
    "    s,p,o = triple\n",
    "\n",
    "    object_augmented = [(x,y,z) for  (x,y), z in itertools.product([triple[0:2]],entities)]\n",
    "    subject_augmented =[(x,y,z) for  x, (y,z) in itertools.product(entities,[triple[1:3]])]\n",
    "    \n",
    "    \n",
    "    return itertools.chain(object_augmented , subject_augmented)\n",
    "\n",
    "def get_all_corrupted_triples(triple,entities):\n",
    "    #too slow ....\n",
    "    \n",
    "    s,p,o = triple\n",
    "    subject_corrupted = [(s_corr,p,o) for s_corr in entities if s_corr != s]\n",
    "    object_corrupted = [(s,p,o_corr)   for o_corr in entities if o_corr != o]\n",
    "\n",
    "    return subject_corrupted + object_corrupted\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def choose_many_multiple(arrs,n):\n",
    "    l = len(arrs[0])\n",
    "    for a in arrs:\n",
    "        assert len(a) == l, 'Arres not of same length ! :('\n",
    "        \n",
    "    \n",
    "    ix = np.random.choice(range(len(a)),n)\n",
    "    \n",
    "    return [np.array(a)[ix] for a in arrs]\n",
    "    \n",
    "def choose_many(a,n):\n",
    "    ix = np.random.choice(range(len(a)),n)\n",
    "    return np.array(a)[ix]\n",
    "    \n",
    "def choose(a):\n",
    "\n",
    "    L = len(a)\n",
    "\n",
    "    i = np.random.randint(0,L)\n",
    "\n",
    "    return a[i]\n",
    "\n",
    "def get_random_corrupted_triple(triple,entities, corrupt='object'):\n",
    "    \"\"\"\n",
    "    corrupt = one of 'subject', 'object', 'both'\n",
    "    \n",
    "    return corrupted triple with random entity\n",
    "    \"\"\"\n",
    "\n",
    "    s,p,o = triple\n",
    "    \n",
    "    # set up as the same\n",
    "    s_corr = s\n",
    "    o_corr = o\n",
    "    \n",
    "    if corrupt == 'subject':  \n",
    "        # corrupt only the subject\n",
    "        while s_corr == s:\n",
    "            s_corr = choose(entities)  \n",
    "    elif corrupt == 'object':\n",
    "        # corrupt only the object\n",
    "        while o_corr == o:\n",
    "            o_corr = choose(entities)  \n",
    "    elif corrupt == 'random':\n",
    "        # corrupt one or both randomly\n",
    "        ch = np.random.randint(3)\n",
    "        \n",
    "        if ch == 0:\n",
    "            while s_corr == s:\n",
    "                s_corr = choose(entities)  \n",
    "        if ch == 1 :\n",
    "            while o_corr == o:\n",
    "                o_corr = choose(entities)  \n",
    "        if ch == 2:\n",
    "            while s_corr == s or o_corr == o:\n",
    "                s_corr = choose(entities)  \n",
    "                o_corr = choose(entities) \n",
    "    else:\n",
    "        while s_corr == s or o_corr == o:\n",
    "            s_corr = choose(entities)  \n",
    "            o_corr = choose(entities) \n",
    "            \n",
    "    \n",
    "    return (s_corr,p,o_corr)\n",
    "    \n",
    "def merge_historires(history_list):\n",
    "    h = {}\n",
    "    for key in history_list[0].history.keys():\n",
    "        h[key] = [h.history[key][0] for h in histories]\n",
    "    return h    \n",
    "\n",
    "\n",
    "def clean_graph(graph,wv):\n",
    "    \"\"\"\n",
    "    clean graph such that all triples have word vectors present in wv\n",
    "    \n",
    "    \"\"\"\n",
    "    no_removed = 0 \n",
    "    for t in graph:\n",
    "        s,p,o = t\n",
    "        if not str(s) in wv.key_to_index.keys() or not str(p) in wv.key_to_index.keys() or not str(o) in wv.key_to_index.keys():\n",
    "            graph.remove(t)\n",
    "            no_removed+=1\n",
    "    return no_removed\n",
    "    \n",
    "    \n",
    "def get_vectors_fast(triples,entity_vec_mapping,vector_size=VECTOR_SIZE):\n",
    "    # ~20-30% faster\n",
    "    X = np.array(triples)\n",
    "    X = word_vectors[X.flatten()].reshape(len(triples),vector_size*3)\n",
    "    \n",
    "    return X    \n",
    "\n",
    "def get_vectors(triples,entity_vec_mapping,vector_size=200):\n",
    "    X = np.array(triples)\n",
    "    X = [(entity_vec_mapping(x[0]), entity_vec_mapping(x[1]),entity_vec_mapping(x[2])) for x in X]\n",
    "    X = [np.concatenate(x) for x in X]\n",
    "    X = np.vstack(X).astype(np.float64)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_1_1_dataset(graph, entities,entity_vec_mapping,corrupt='random'):\n",
    "    \n",
    "    original_triple_len = len(graph)\n",
    "    # get triples\n",
    "    X = list(graph)\n",
    "    no_t = len(X)\n",
    "    \n",
    "\n",
    "    \n",
    "    corrupted_triples = [get_random_corrupted_triple(x,entities,corrupt=corrupt) for x in X]\n",
    "    X = X + corrupted_triples\n",
    "    \n",
    "    \n",
    "\n",
    "    # convert uris to strings\n",
    "    \n",
    "    X = get_vectors_fast(X,entity_vec_mapping)\n",
    "    \n",
    "    # stack them\n",
    "\n",
    "    Y = np.concatenate((np.ones(no_t),np.zeros(no_t))).astype(np.uint8)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def test_sklearn_model(model,X,Y,x_test,y_test,subset=10000):\n",
    "    \n",
    "\n",
    "  \n",
    "    \n",
    "    ix = np.random.choice(range(len(X)),size=subset)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    \n",
    "    X_scaled = scaler.transform(X[ix])\n",
    "    model.fit(X_scaled,Y[ix])\n",
    "\n",
    "    print(f'train_score ={model.score(scaler.transform(X),Y)}')    \n",
    "    print(f'test_score ={model.score(scaler.transform(x_test),y_test)}')\n",
    "\n",
    "def scale_and_predict(model,x):\n",
    "    x = preprocessing.StandardScaler().fit_transform(x)\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97199e0-c550-49f1-a095-5ef70d0497d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word_vectors = Word2Vec.load('walks/model').wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187391ce-a626-47f8-8f9f-104f3f2a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keyed_vectors(word_vectors, iterable):\n",
    "    \"\"\"\n",
    "    for some reason faster than native call :O\n",
    "    \"\"\"\n",
    "    return np.array(list(word_vectors.get_vector(x) for x in iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0a6e85-44a1-469d-ae05-1e9b56425bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-12 12:42:14.545081: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-12 12:42:14.545100: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# pytorch model\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class ClassifierSimple(torch.nn.Module):\n",
    "    def __init__(self,input_dim=300,hidden_size=64):\n",
    "        super(ClassifierSimple, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "                # flatten input if necessary\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(input_dim,hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size,1)\n",
    "        )\n",
    "        \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "                \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "        \n",
    "    \n",
    "    def forward(self,x):        \n",
    "        \n",
    "        return self.layers(x)\n",
    "    def predict(self,x):\n",
    "        x.to(self.device)\n",
    "        \n",
    "        return self.output_activation(self.layers(x))\n",
    "    def predict_numpy(self,x):\n",
    "        x = torch.tensor(x)\n",
    "        x.to(self.device)\n",
    "        return self.output_activation(self.layers(x)).detach().cpu().numpy()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0810bf13-53a4-4829-b1af-6137f2e2029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found trained model! Loading :)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "model = ClassifierSimple()    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if Path('rdf2vecClassfier.pth').is_file():\n",
    "    print('found trained model! Loading :)')\n",
    "    model.load_state_dict(torch.load('rdf2vecClassfier.pth'))\n",
    "    history = pd.read_csv('log.csv')\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    print('model not found. Train it with ''train_rdf2vec_classifier.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95ab1b2-1076-4550-9e28-71659f588af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f117e933-8fc0-45b4-8ad0-32ab312e474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_concat(se,pe,oe):\n",
    "    \n",
    "    assert se.shape == pe.shape, \"Error! fast_concat with differing shapes\"\n",
    "    assert se.shape == oe.shape, \"Error! fast_concat with differing shapes\"\n",
    "    \n",
    "    \n",
    "    x = np.empty((se.shape[0],se.shape[1]*3),dtype=np.float32)\n",
    "    x[:,0:100] =se\n",
    "    x[:,100:200] = pe\n",
    "    x[:,200:] = oe\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaff9a5d-fb1f-4882-8d62-f77d2603e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_s_o_tuples(entities):\n",
    "    return [(s,o) for s in entities for o in entities] + [(o,s) for s in entities for o in entities]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea8294f-161a-4ec6-9335-46eeae2bee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_if_row_in_array(row,arr):\n",
    "    return np.any(np.sum(arr == row,axis=1) == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e886ad23-0789-497f-9e0b-040423a16648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter(triples,graphs):\n",
    "    # Too slow ... parallelize?! What else? Faster lookup? How?\n",
    "    graphs = [np.array(g) for g in graphs]\n",
    "\n",
    "    graphs = np.concatenate(graphs,axis=0)\n",
    "    \n",
    "    known_ix = []\n",
    "    \n",
    "    for i,tp in enumerate(triples):\n",
    "        if test_if_row_in_array(tp,graphs):\n",
    "            known_ix.append(i)\n",
    "    return known_ix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba56b73e-b86d-40b1-84d1-9e5779922b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keyed_vectors(word_vectors, iterable):\n",
    "    \"\"\"\n",
    "    for some reason faster than native call :O\n",
    "    \"\"\"\n",
    "    return np.array(list(word_vectors.get_vector(x) for x in iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac97b25b-2378-44a5-ba8a-40d13a74a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.utils.extmath import cartesian\n",
    "\n",
    "def evaluate_link_pred_fast(score_f,graph,entity_vec_mapping,entities,vector_size = 100, max_triples=100, plot = False, filter_by=None,verbose = True):\n",
    "    \n",
    "    \n",
    "    \n",
    "    stats = {'preprocessing_time' : -1,\n",
    "        'embeddings_time': [],\n",
    "        'rank_time': [],\n",
    "        'find_rank_time':[],\n",
    "        'ranks':[]\n",
    "        }\n",
    "    \n",
    "    start_timer = time.perf_counter()\n",
    "    predicates = np.array(list(set(graph.predicates())))\n",
    "    \n",
    "    graph = np.array(graph)\n",
    "    \n",
    "    \n",
    "    print(f\"evaluate LP on graph with {len(graph)} triples, {len(entities)} entities and {len(predicates)} predicates!\")\n",
    "    \n",
    "    print(f\"Starting preprocessing\")\n",
    "    embeddings_scores = defaultdict(lambda: defaultdict(lambda :None))\n",
    "    \n",
    "    entity_array = np.array(entities)\n",
    "    entity_mapping = entity_vec_mapping(np.array(entity_array))\n",
    "    \n",
    "    no_entitites= len(entities)\n",
    "    ix = list(range(no_entitites))\n",
    "    \n",
    "    s_o_combinations = cartesian((ix,ix))   \n",
    "\n",
    "    subjects_ix = s_o_combinations[:,0] # sorted(s_o_combinations[:,0])\n",
    "    subject_embeddings = entity_mapping[subjects_ix]\n",
    "                                 \n",
    "\n",
    "    \n",
    "    objects_ix = s_o_combinations[:,1]  #sorted(s_o_combinations[:,1])\n",
    "    object_embeddings = entity_mapping[objects_ix]\n",
    "    \n",
    "                           \n",
    "    no_triples_per_predicate=len(subjects_ix)\n",
    "    \n",
    "    \n",
    "    \n",
    "    preprocessing_timer = time.perf_counter()\n",
    "    \n",
    "    stats['preprocessing_time'] = preprocessing_timer - start_timer\n",
    "    \n",
    "    print(f\"Finished preprocessing\")\n",
    "    \n",
    "    # del is very slow :(\n",
    "    # del s_o_combinations\n",
    "    s_o_combinations = None\n",
    "    #gc.collect()\n",
    "    \n",
    "    \n",
    "    for p in tqdm(predicates):\n",
    "        predicate_start_timer = time.perf_counter()\n",
    "        \n",
    "        \n",
    "             \n",
    "        predicate_embedding = entity_vec_mapping([p])\n",
    "        predicate_column = np.repeat(predicate_embedding,no_triples_per_predicate).reshape(no_triples_per_predicate,vector_size)\n",
    "        \n",
    "        #return subject_embeddings,predicate_column,object_embeddings\n",
    "\n",
    "        triple_embeddings = fast_concat(subject_embeddings,predicate_column,object_embeddings)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        predicate_embeddings_timer = time.perf_counter()\n",
    "        \n",
    "        #del predicate_column\n",
    "        predicate_column = None\n",
    "\n",
    "        \n",
    "\n",
    "        scores = np.squeeze(score_f(triple_embeddings))\n",
    "        \n",
    "        #del triple_embeddings\n",
    "        triple_embeddings = None\n",
    "        #gc.collect()\n",
    "        \n",
    "        \n",
    "        \n",
    "        sorted_ix = np.flip(np.argsort(scores))     \n",
    "        \n",
    "        subjects = entity_array[subjects_ix] \n",
    "        objects = entity_array[objects_ix]\n",
    "    \n",
    "        scored_triples = np.stack([subjects,np.repeat(p,no_triples_per_predicate),objects]).T   \n",
    "        scored_triples = scored_triples[sorted_ix]\n",
    "        \n",
    "        \n",
    "        predicate_rank_timer = time.perf_counter()\n",
    "        \n",
    "        ranks = []\n",
    "    \n",
    "\n",
    "        predicate_subgraph = graph[graph[:,1] == p]\n",
    "    \n",
    "    \n",
    "        \n",
    "        for triple in predicate_subgraph:\n",
    "            try:\n",
    "                rank = np.where(np.sum(scored_triples ==triple,axis=1) == 3)[0][0]\n",
    "                print(rank)\n",
    "                ranks.append(rank)\n",
    "            except:\n",
    "                print(triple)\n",
    "                print(np.where(np.sum(scored_triples ==triple,axis=1)))\n",
    "                print('unknown entity or relation!')\n",
    "                \n",
    "        \n",
    "        \n",
    "        predicate_find_rank = time.perf_counter()\n",
    "        \n",
    "            \n",
    "        stats['embeddings_time'].append(predicate_embeddings_timer-predicate_start_timer)\n",
    "        stats['rank_time'].append(predicate_rank_timer - predicate_embeddings_timer)\n",
    "        stats['find_rank_time'].append(predicate_find_rank - predicate_rank_timer)\n",
    "        stats['ranks'].extend(ranks)\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "    return  stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b81ef830-ee02-4b97-93bd-9e6fe2ab7f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 0 triples from training set\n",
      "removed 0 triples from validation set\n",
      "removed 0 triples from test set\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "\n",
    "\n",
    "\n",
    "g_train = Graph()\n",
    "g_val = Graph()\n",
    "g_test = Graph()\n",
    "\n",
    "g_train = g_train.parse('FB15k-237/train_100.nt', format='nt')\n",
    "g_val   = g_val.parse('FB15k-237/valid_100.nt', format='nt')\n",
    "g_test  = g_test.parse('FB15k-237/test_100.nt', format='nt')\n",
    "\n",
    "\n",
    "# clean graphs \n",
    "# number of triples removed should be low, a few hundred\n",
    "print(f\"removed {clean_graph(g_train,word_vectors)} triples from training set\")\n",
    "print(f\"removed {clean_graph(g_val,word_vectors)} triples from validation set\")\n",
    "print(f\"removed {clean_graph(g_test,word_vectors)} triples from test set\")\n",
    "\n",
    "entities = get_entities((g_train,g_val,g_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bcfeb3c-d41d-4eee-848c-7af559960d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank(scores,ix,mask=None):\n",
    "    if mask == None:    \n",
    "        optimistic_rank =(scores > scores[ix]).sum()+1\n",
    "        pessimistic_rank = (scores >= scores[ix]).sum()\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        optimistic_rank = ((scores > scores[ix]).index_fill(0,mask,False)).sum()+1\n",
    "        pessimistic_rank = ((scores >= scores[ix]).index_fill(0,mask,False)).sum()\n",
    "        \n",
    "    rank = (optimistic_rank+pessimistic_rank)*0.5\n",
    "        \n",
    "    return rank\n",
    "\n",
    "def parse_rdflib_to_torch(graph):\n",
    "    entities = get_entities([graph])\n",
    "    \n",
    "    entities=np.array(entities)\n",
    "    entity_vecs= torch.tensor(word_vectors[np.array(entities)])\n",
    "    entities = dict(zip(entities,range(len(entities))))\n",
    "    \n",
    "    predicates = np.array(list(set(graph.predicates())))\n",
    "    predicate_vecs = torch.tensor(word_vectors[predicates])\n",
    "    predicates = dict(zip(predicates,range(len(predicates))))\n",
    "    \n",
    "    edges = []\n",
    "    predicate_ix = []\n",
    "    for s,p,o in np.array(graph):\n",
    "        try:\n",
    "            edge = (entities[s],entities[o])\n",
    "            edges.append(edge)\n",
    "            predicate_ix.append(predicates[p])\n",
    "        except:\n",
    "            print(f\"Unknown entities encountered! ({s},{o})\")\n",
    "\n",
    "    edges = torch.tensor(edges)\n",
    "    predicate_ix = torch.tensor(predicate_ix)\n",
    "    \n",
    "    return edges, predicate_ix, entities, predicates,entity_vecs,predicate_vecs\n",
    "\n",
    "def get_comb_ix(edge,no_entities):\n",
    "    return edge[0]*no_entities+edge[1]\n",
    "\n",
    "def create_mask(length,ix,reverse=False):\n",
    "    if not reverse:\n",
    "        mask = torch.ones(length)\n",
    "        mask = mask.index_fill(0,ix,0)\n",
    "    else:\n",
    "        mask = torch.zeros(length)\n",
    "        mask = mask.index_fill(0,ix,1)\n",
    "    \n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04e2ea19-599e-430b-9b92-41712b5f5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timer import PerfTimer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def eval_ranks(model,graph,filtered = True, batchsize=None, vecsize=100,force_cpu=False):\n",
    "# get data from graph\n",
    "    \n",
    "    \n",
    "    ### Setup ###\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if force_cpu:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    perfTimer = PerfTimer()\n",
    "    \n",
    "    perfTimer.start()\n",
    "    \n",
    "    ### Preprocessing graph into torch arrays and ix\n",
    "    edges, predicate_ix, entity_ix_mapping, predicate_ix_mapping,entity_vecs,predicate_vecs = parse_rdflib_to_torch(graph)\n",
    "   \n",
    "    \n",
    "    ranks = []\n",
    "    ix_predicate_mapping = {k:v for (v,k) in predicate_ix_mapping.items()}\n",
    "    ix_entity_mapping = {k:v for (v,k) in entity_ix_mapping.items()}\n",
    "    \n",
    "    no_entities  = len(entity_ix_mapping.keys())\n",
    "    \n",
    "    perfTimer.track('preprocessing')  \n",
    "    \n",
    "    # could also try torch.cross and torch.combinations if needed\n",
    "    # all possible s_o_combinations\n",
    "    s_o_combs = torch.tensor(cartesian((range(no_entities),range(no_entities))))\n",
    "    perfTimer.track('combinations')  \n",
    "    # Allocate array only once\n",
    "    \n",
    "    if not batchsize:\n",
    "        # Full array in RAM\n",
    "        to_score_embeddings = torch.empty((len(s_o_combs),vecsize*3)).to(device)\n",
    "               \n",
    "        to_score_embeddings[:,0:vecsize] =  entity_vecs[s_o_combs[:,0]]\n",
    "        to_score_embeddings[:,2*vecsize:]  = entity_vecs[s_o_combs[:,1]]          \n",
    "    else:\n",
    "        # Allocate Array of (Batchsize,dim)\n",
    "        to_score_embeddings = torch.empty((batchsize,vecsize*3)).to(device)\n",
    "        \n",
    "        \n",
    "    for pred_ix in tqdm(ix_predicate_mapping.keys()):\n",
    "        # loop over all predicates\n",
    "        \n",
    "        current_edges = edges[predicate_ix == pred_ix]\n",
    "        number_of_real_edges = len(current_edges)\n",
    "        \n",
    "        perfTimer.track('subgraph')\n",
    "        \n",
    "\n",
    "        \n",
    "        edge_ix = torch.tensor([get_comb_ix(x,no_entities) for x in current_edges])\n",
    "        perfTimer.track('edges')\n",
    "        \n",
    "        if batchsize:\n",
    "            dl = DataLoader(s_o_combs, batch_size=batchsize, shuffle=False)\n",
    "            perfTimer.track('dl')\n",
    "            predicate_embedding = predicate_vecs[pred_ix]\n",
    "            \n",
    "            #use expand as memory of rows is shared. This may cause bugs ... investigate!\n",
    "            predicate_embedding = predicate_embedding.reshape(1,vecsize).expand(batchsize,vecsize)\n",
    "            perfTimer.track('expand_pediacte')\n",
    "            \n",
    "            scores = []\n",
    "\n",
    "            for batch in dl:\n",
    "                #print(s_o_combs[:,0].type())\n",
    "                #print(batch[:,0].type())\n",
    "                if len(batch) != batchsize:\n",
    "                    \n",
    "                    to_score_embeddings[:,0:vecsize][0:len(batch)] = entity_vecs[batch[:,0]]\n",
    "                    to_score_embeddings[:,2*vecsize:][0:len(batch)]  = entity_vecs[batch[:,1]]  \n",
    "                    \n",
    "                    to_score_embeddings[:,vecsize:2*vecsize] =  predicate_embedding\n",
    "                    \n",
    "                    perfTimer.track('copy embeddings into array')\n",
    "\n",
    "                    to_score_embeddings = to_score_embeddings.to(device)\n",
    "                    perfTimer.track('to_device')\n",
    "                    batch_scores = model(to_score_embeddings)\n",
    "                    perfTimer.track('predict_batch')\n",
    "                    batch_scores = batch_scores[0:len(batch)]\n",
    "                    scores.append(batch_scores)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    to_score_embeddings[:,0:vecsize] =  entity_vecs[batch[:,0]]\n",
    "                    to_score_embeddings[:,2*vecsize:]  = entity_vecs[batch[:,1]]    \n",
    "                    to_score_embeddings[:,vecsize:2*vecsize] =  predicate_embedding\n",
    "                    \n",
    "                    perfTimer.track('copy embeddings into array')\n",
    "\n",
    "                    to_score_embeddings = to_score_embeddings.to(device)\n",
    "                    perfTimer.track('to_device')\n",
    "                    batch_scores = model(to_score_embeddings)\n",
    "                    perfTimer.track('predict_batch')\n",
    "                    scores.append(batch_scores)\n",
    "                \n",
    "            scores = torch.vstack(scores).squeeze()\n",
    "            perfTimer.track('stack_all')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "           \n",
    "\n",
    "            predicate_embedding = predicate_vecs[pred_ix]\n",
    "            #use expand as memory of rows is shared. This may cause bugs ... investigate!\n",
    "            predicate_embedding = predicate_embedding.reshape(1,100).expand(len(to_score_embeddings),100)\n",
    "            \n",
    "            perfTimer.track('collect_embeddings')\n",
    "\n",
    "            to_score_embeddings[:,vecsize:2*vecsize] =  predicate_embedding\n",
    "            perfTimer.track('stack_embeddings')\n",
    "\n",
    "            \n",
    "            to_score_embeddings = to_score_embeddings.to(device)\n",
    "            perfTimer.track('to_device')\n",
    "            scores = model(to_score_embeddings).squeeze()\n",
    "            perfTimer.track('score_embeddings')\n",
    "            \n",
    "        #sorted_ix = scor\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        for ix in edge_ix:\n",
    "            ranks.append(compute_rank(scores,ix,edge_ix.to(device)))\n",
    "        perfTimer.track('rank_embeddings')\n",
    "        \n",
    "    return torch.tensor(ranks), perfTimer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e69ef853-2962-422b-bcb5-bd6907242f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f707a5c-5794-4454-b0f1-3aea83d23d82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:09<00:00,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(104.5900)\n",
      "tensor(0.1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ranks, pt = eval_ranks(model,g_train,force_cpu = True,batchsize=100000)\n",
    "    print(ranks.mean())\n",
    "    print((1/ranks).mean())\n",
    "gc.collect()\n",
    "#pt.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5fa35364-5cda-4f30-ac38-41ba11e00aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocessing': 0.002771381001366535,\n",
       " 'combinations': 0.00034103700090781786,\n",
       " 'subgraph': 0.04503918501723092,\n",
       " 'edges': 0.0038463269811472856,\n",
       " 'dl': 0.002323473985597957,\n",
       " 'expand_pediacte': 0.001306367001234321,\n",
       " 'collect_embeddings_batch': 5.62810077299946,\n",
       " 'to_device': 0.0009826520217757206,\n",
       " 'predict_batch': 4.13666238497899,\n",
       " 'stack_all': 0.02201943202089751,\n",
       " 'rank_embeddings': 0.048736660992290126}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tested on cpu\n",
    "pt.stats_sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std-env",
   "language": "python",
   "name": "std-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
